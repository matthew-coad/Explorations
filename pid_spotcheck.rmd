---
title: "Pima Indians Diabetes - Spotcheck"
output:
  html_document:
    df_print: paged
---

```{r, include=FALSE}
library(tidyverse)
library(rlang)
library(caret)
library(emc2)
library(recipes)
library(forcats)

knitr::opts_chunk$set(fig.width = 14, fig.height = 8, echo = FALSE, warning = FALSE, message = FALSE, comment = "")
options(width = 120)

source("./pid_load.R")
```

# Variable Strategy

## Pregnant

None

## Glucose

Only two missing values. Consider "reasonably complete".
## Pressure

30 missing values. Consider "reasonably complete".

## Mass

9 missing values. Consider "reasonably complete".

## Triceps

185 missing values. Consider "incomplete".

## insulin

301 missing values. Consider "incomplete".

```{r}
quick_spot_df <- pid_load() %>% pid_holdout() %>% pid_clean()
numeric_cols <- quick_spot_df %>% select_if(is.numeric) %>% colnames() %>% syms()
complete_cols <- c("pregnant", "pedigree", "age")
reasonable_complete_cols <- c("glucose", "mass", "pressure")
incomplete_cols <- c("triceps", "insulin")

```

# Quick Spotcheck Algorithms

## Strategy

Standardize variables.

Impute reasonably complete values with median.

Impute incomplete values with bag imputation.

## View algorithms

```{r}

emc_algorithms() %>% filter(two_class, core)

```

Try 

```{r}
quick_spot_algorithms <- c("null", "earth", "glmnet", "lda", "nb", "ada", "rf", "svmLinear", "svmRadial", "xgbLinear", "xgbTree")
paste(quick_spot_algorithms, collapse = ", ") %>% cat()
```

## Spotcheck parameters

View the spotcheck parameters

```{r}
quick_spot_parameters <- emc_parameters(name = sprintf("Quick spotcheck '%s'", algorithm),
                                        algorithm = quick_spot_algorithms)
quick_spot_parameters
```

## Spotcheck emc

```{r}
quick_spot_metric <- "Accuracy"
quick_spot_control <- trainControl(method = "repeatedcv", number = 5, repeats = 5)
quick_spot_recipe <- recipe(diabetes ~ ., quick_spot_df) %>% 
    step_meanimpute(reasonable_complete_cols) %>%
    step_scale(all_predictors()) %>%
    step_center(all_predictors()) %>%
    step_BoxCox(all_predictors()) %>%
    step_bagimpute(triceps) %>%
    step_bagimpute(insulin)


if (!exists("prior_quick_spot_emc")) {
    prior_quick_spot_emc <- NULL
}

# prior_quick_spot_emc <- quick_spot_emc

quick_spot_train <-
    emc_train(
        quick_spot_parameters,
        quick_spot_recipe,
        data = quick_spot_df,
        method = algorithm,
        trControl = quick_spot_control,
        metric = quick_spot_metric,
        verbose = FALSE,
        cache_path = paste0("Cache/", name, ".rds")
    )

emc_performance(quick_spot_train)
```

## Whats up with Naive Bayes?

```{r}

quick_spot_train %>% filter(algorithm == "nb") %>% pull(log) %>% emc_replay()

```

Its having trouble making predictions in some cases. We won't use it.

## Accuracy Boxplot

```{r}

quick_spot_resamples_data <- emc_resamples_data(quick_spot_train)
quick_spot_resamples_data %>%
    ggplot() +
    geom_boxplot(aes(x = fct_reorder(algorithm, accuracy), y = accuracy))
    
```

Looks like the algorithms are maxing out at about .77 performance. Linear Discrimant analysis is a simple linear algorithm and is doing pretty well.

## Accuracy Density Plot

```{r}
quick_spot_resamples_data %>%
    filter(algorithm != "null") %>%
    ggplot() +
    geom_density(aes(x = accuracy, color = algorithm))
```

## Accuracy Differences

```{r}
quick_spot_resamples <- 
    quick_spot_train %>% 
    filter(!algorithm %in% c("null", "nb", "xgbLinear")) %>% 
    emc_resamples(name = algorithm)
summary(diff(quick_spot_resamples))
```

Resample differences confirm their is nothing between the top algorithms

## Resamples Bar

```{r}
ggplot(quick_spot_resamples)
```

## Resamples Scatter

```{r}

parallelplot(quick_spot_resamples)

```

Worth considering stacking.

After checking it doesn't! Lets go with lda. Its a simple linear model.

Get the final model.

```{r}
final_model <- quick_spot_train %>% filter(algorithm == "lda") %>% pull(result) %>% first()
final_model
```

But lets just grab the final model and validate it.

```{r}
test_df <- pid_training(pid_load())
test_predictions <- predict(final_model, newdata = test_df)
caret::confusionMatrix(test_predictions, test_df$diabetes)

```

76.5% accuracy with the 95% CI inside our 77% target.

